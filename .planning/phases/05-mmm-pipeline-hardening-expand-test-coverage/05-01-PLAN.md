---
phase: 05-mmm-pipeline-hardening-expand-test-coverage
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/singular/test_mmm_date_spine_completeness.sql
  - tests/singular/test_mmm_cross_layer_consistency.sql
  - tests/singular/test_mmm_zero_fill_integrity.sql
  - analysis/check_network_mapping_coverage.sql
autonomous: true

must_haves:
  truths:
    - "Date spine completeness test exists and will catch any missing date+channel+platform gaps"
    - "Cross-layer consistency test exists and will catch aggregation mismatches between intermediate and mart layers"
    - "Zero-fill integrity test exists and will catch HAS_*_DATA flag mismatches"
    - "Network mapping coverage analysis query exists to identify unmapped partners"
  artifacts:
    - path: "tests/singular/test_mmm_date_spine_completeness.sql"
      provides: "Singular test for date spine gap detection"
      contains: "mmm__daily_channel_summary"
    - path: "tests/singular/test_mmm_cross_layer_consistency.sql"
      provides: "Singular test for intermediate-to-mart totals consistency"
      contains: "int_mmm__daily_channel_spend"
    - path: "tests/singular/test_mmm_zero_fill_integrity.sql"
      provides: "Singular test for HAS_*_DATA flag correctness"
      contains: "HAS_SPEND_DATA"
    - path: "analysis/check_network_mapping_coverage.sql"
      provides: "Analysis query to find unmapped PARTNER_NAME values"
      contains: "network_mapping"
  key_links:
    - from: "tests/singular/test_mmm_date_spine_completeness.sql"
      to: "models/marts/mmm/mmm__daily_channel_summary.sql"
      via: "ref('mmm__daily_channel_summary')"
      pattern: "ref\\('mmm__daily_channel_summary'\\)"
    - from: "tests/singular/test_mmm_cross_layer_consistency.sql"
      to: "models/intermediate/int_mmm__daily_channel_spend.sql"
      via: "ref() to all three intermediate models"
      pattern: "ref\\('int_mmm__daily_channel_(spend|installs|revenue)'\\)"
    - from: "tests/singular/test_mmm_zero_fill_integrity.sql"
      to: "models/marts/mmm/mmm__daily_channel_summary.sql"
      via: "ref('mmm__daily_channel_summary')"
      pattern: "HAS_SPEND_DATA.*HAS_INSTALL_DATA.*HAS_REVENUE_DATA"
---

<objective>
Create three MMM singular tests (date spine completeness, cross-layer consistency, zero-fill integrity) and a network mapping coverage analysis query.

Purpose: These tests validate the MMM pipeline data quality before dbt Cloud execution. The analysis query identifies unmapped partners so the network_mapping seed can be updated before production deployment.

Output: 3 singular test SQL files in tests/singular/ and 1 analysis query in analysis/
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-mmm-pipeline-hardening-expand-test-coverage/05-RESEARCH.md
@models/marts/mmm/mmm__daily_channel_summary.sql
@models/intermediate/int_mmm__daily_channel_spend.sql
@models/intermediate/int_mmm__daily_channel_installs.sql
@models/intermediate/int_mmm__daily_channel_revenue.sql
@models/marts/mmm/_mmm__models.yml
@seeds/network_mapping.csv
@tests/singular/test_ad_partner_mapping_consistency.sql
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create three MMM singular tests</name>
  <files>
    tests/singular/test_mmm_date_spine_completeness.sql
    tests/singular/test_mmm_cross_layer_consistency.sql
    tests/singular/test_mmm_zero_fill_integrity.sql
  </files>
  <action>
Create three singular test SQL files following the pattern established by test_ad_partner_mapping_consistency.sql (zero rows returned = pass, failing rows returned = fail).

**1. test_mmm_date_spine_completeness.sql (TEST-06)**
- Generate expected date range using `{{ dbt_utils.date_spine(datepart="day", start_date="'2024-01-01'", end_date="current_date()") }}`
- Get distinct PLATFORM + CHANNEL combinations from `{{ ref('mmm__daily_channel_summary') }}`
- CROSS JOIN dates with channels to create expected grid
- LEFT JOIN actual data from `{{ ref('mmm__daily_channel_summary') }}`
- Return rows where actual data is NULL (missing combinations)
- Include DATE, PLATFORM, CHANNEL, and error_reason in output columns
- Add SQL header comment explaining what the test validates and when it fails

**2. test_mmm_cross_layer_consistency.sql (TEST-07)**
- Sum SPEND from `{{ ref('int_mmm__daily_channel_spend') }}`, INSTALLS from `{{ ref('int_mmm__daily_channel_installs') }}`, REVENUE from `{{ ref('int_mmm__daily_channel_revenue') }}` at DATE + PLATFORM grain using UNION ALL with zero-padding for non-applicable metrics
- Sum SPEND, INSTALLS, REVENUE from `{{ ref('mmm__daily_channel_summary') }}` at DATE + PLATFORM grain, filtered to `WHERE HAS_SPEND_DATA = 1 OR HAS_INSTALL_DATA = 1 OR HAS_REVENUE_DATA = 1` (excludes purely zero-filled date spine rows)
- FULL OUTER JOIN on DATE + PLATFORM
- Return rows where ABS(intermediate - mart) > 0.01 for SPEND/REVENUE or > 0 for INSTALLS
- Include both intermediate and mart values plus error_type column for diagnostics
- Use COALESCE with 0 for NULL handling in comparisons

**3. test_mmm_zero_fill_integrity.sql (TEST-08)**
- Query all rows from `{{ ref('mmm__daily_channel_summary') }}`
- Check six violation conditions:
  - SPEND > 0 AND HAS_SPEND_DATA = 0 (has data but flag says no)
  - SPEND = 0 AND HAS_SPEND_DATA = 1 (no data but flag says yes)
  - Same two checks for INSTALLS / HAS_INSTALL_DATA
  - Same two checks for REVENUE / HAS_REVENUE_DATA
- Return rows with any violation, including DATE, PLATFORM, CHANNEL, the violation_type, and the actual metric + flag values for debugging
- Note in header comment: This test validates that the CASE WHEN IS NOT NULL logic in the mart correctly sets HAS_*_DATA flags

Each test file must:
- Start with a SQL comment block explaining what it tests, why it matters, and what "fail" means
- Use {{ ref() }} for all model references (no hardcoded table names)
- Return zero rows on success (dbt singular test convention)
- Include enough columns in output for debugging failures
  </action>
  <verify>
Verify all three files exist and contain valid Jinja SQL:
- Each file uses {{ ref() }} (not hardcoded table names)
- Each file uses {{ dbt_utils.date_spine() }} where appropriate
- No syntax errors in SQL (balanced parentheses, valid CASE/WHEN, proper CTEs)
- Each file has a header comment block
  </verify>
  <done>
Three singular test files exist in tests/singular/ that will validate: (1) date spine has no gaps for any date+channel+platform combination, (2) intermediate layer totals match mart totals within tolerance, (3) HAS_*_DATA flags correctly match metric presence. All use ref() and follow existing test conventions.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create network mapping coverage analysis query</name>
  <files>
    analysis/check_network_mapping_coverage.sql
  </files>
  <action>
Create an analysis SQL file (NOT a test -- goes in analysis/ directory, not tests/) that identifies unmapped PARTNER_NAME values across all source models.

The query should:

1. Collect distinct PARTNER_NAME values from Supermetrics spend data (last 90 days, where COST > 0):
   `{{ ref('stg_supermetrics__adj_campaign') }}` with DATE >= DATEADD(day, -90, CURRENT_DATE)

2. Collect distinct PARTNER_NAME values from Adjust API revenue data (last 90 days, where REVENUE > 0 or INSTALLS > 0):
   `{{ ref('stg_adjust__report_daily') }}` with DATE >= DATEADD(day, -90, CURRENT_DATE)

3. UNION both sets with a `source` column indicating which source each came from

4. LEFT JOIN to `{{ ref('network_mapping') }}` on BOTH:
   - SUPERMETRICS_PARTNER_NAME (for Supermetrics source)
   - ADJUST_NETWORK_NAME (for Adjust API source)

5. Return a summary showing:
   - coverage_status: 'Mapped' or 'UNMAPPED - will map to Other'
   - partner_count: count of distinct partners per status
   - partners: LISTAGG of partner names for easy reading

6. Also include a detail section showing each unmapped partner with its source and impact

Add header comment: "Run with: dbt compile --select check_network_mapping_coverage, then execute compiled SQL in Snowflake worksheet to see results"

Create the analysis/ directory if it does not exist.
  </action>
  <verify>
- File exists at analysis/check_network_mapping_coverage.sql
- Uses {{ ref() }} for all model references
- Queries both stg_supermetrics__adj_campaign and stg_adjust__report_daily
- Joins to network_mapping seed
- Has header comment with usage instructions
  </verify>
  <done>
Analysis query exists that will identify all active PARTNER_NAME values not covered by the network_mapping seed, grouped by coverage status. Ready to be compiled and executed in dbt Cloud to discover gaps before production deployment.
  </done>
</task>

</tasks>

<verification>
All four SQL files exist and follow dbt conventions:
- `ls tests/singular/test_mmm_*.sql` shows 3 files
- `ls analysis/check_network_mapping_coverage.sql` shows 1 file
- All files use `{{ ref() }}` syntax (grep for "ref(" in each file)
- No files use hardcoded table names
</verification>

<success_criteria>
- 3 singular test files created matching TEST-06, TEST-07, TEST-08 requirements
- 1 analysis query created for network mapping coverage (MMM-03 prep)
- All files follow established project conventions (ref(), header comments, zero-rows-pass pattern)
- Ready for dbt Cloud compilation and execution in Plan 02
</success_criteria>

<output>
After completion, create `.planning/phases/05-mmm-pipeline-hardening-expand-test-coverage/05-01-SUMMARY.md`
</output>
